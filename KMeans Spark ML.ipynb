{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este modelo, se busca encontrar la cantidad de **hackers** en base a la cantidad de hackeos. Se clasifican mediante **KMeans**.\n",
    "\n",
    "[Dataset](https://www.dropbox.com/s/g5r2dh46abx1vdr/hack_data.csv?dl=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T21:05:15.252948Z",
     "start_time": "2019-08-14T21:05:15.244975Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('find_hacker').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T21:05:16.364728Z",
     "start_time": "2019-08-14T21:05:15.977457Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Session_Connection_Time: double (nullable = true)\n",
      " |-- Bytes Transferred: double (nullable = true)\n",
      " |-- Kali_Trace_Used: integer (nullable = true)\n",
      " |-- Servers_Corrupted: double (nullable = true)\n",
      " |-- Pages_Corrupted: double (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- WPM_Typing_Speed: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    " \n",
    "dataset = spark.read.csv(\"./hack_data.csv\",header=True,inferSchema=True)\n",
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se transforman las columnas de características (*features*) a 1 sola columna con **VectorAssembler**, al cual se le pasan las columnas de *input* (**feat_cols**) y entrega la columna *output* con todos los valores de las columnas entrantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T21:05:18.724167Z",
     "start_time": "2019-08-14T21:05:18.528617Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+\n",
      "|Session_Connection_Time|Bytes Transferred|Kali_Trace_Used|Servers_Corrupted|Pages_Corrupted|            Location|WPM_Typing_Speed|            features|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+\n",
      "|                    8.0|           391.09|              1|             2.96|            7.0|            Slovenia|           72.37|[8.0,391.09,1.0,2...|\n",
      "|                   20.0|           720.99|              0|             3.04|            9.0|British Virgin Is...|           69.08|[20.0,720.99,0.0,...|\n",
      "|                   31.0|           356.32|              1|             3.71|            8.0|             Tokelau|           70.58|[31.0,356.32,1.0,...|\n",
      "+-----------------------+-----------------+---------------+-----------------+---------------+--------------------+----------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    " \n",
    "feat_cols = ['Session_Connection_Time', 'Bytes Transferred', 'Kali_Trace_Used',\n",
    "'Servers_Corrupted', 'Pages_Corrupted','WPM_Typing_Speed']\n",
    " \n",
    "vec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\n",
    " \n",
    "final_data = vec_assembler.transform(dataset)\n",
    "final_data.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se estandarizan las columnas *features*, removiendo la media y escalando a la unidad de la varianza. Así, los datos están normalmente distribuidos.\n",
    "\n",
    "Ahora con los *features* estandarizados, se prodece a hacer **fit** a los datos y luego **transform**.\n",
    "\n",
    "El modelo de clasificación de **KMeans** se le entregan las columnas *features* y la cantidad de agrupaciones (**k**)\n",
    "\n",
    "**KMeans**(<span style=\"color:green;\">featuresCol='features'</span>, predictionCol='prediction', <span style=\"color:green;\">k=2</span>, initMode='k-means||', initSteps=2, tol=0.0001, maxIter=20, <span style=\"color:darkolivegreen;\">seed=None</span>, distanceMeasure='euclidean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T21:05:21.229747Z",
     "start_time": "2019-08-14T21:05:21.062469Z"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    " \n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)\n",
    "\n",
    "scalerModel = scaler.fit(final_data)\n",
    " \n",
    "cluster_final_data = scalerModel.transform(final_data)\n",
    " \n",
    "kmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\n",
    "kmeans2 = KMeans(featuresCol='scaledFeatures',k=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calcula la suma de las diferencias al cuadrado entre cada observación y la media de su grupo, para **k = 2** y **k = 3**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T21:05:23.856280Z",
     "start_time": "2019-08-14T21:05:22.647296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=3\n",
      "Within Set Sum of Squared Errors = 434.1492898715845\n",
      "------------------------------------------------------------\n",
      "With K=2\n",
      "Within Set Sum of Squared Errors = 601.7707512676716\n"
     ]
    }
   ],
   "source": [
    "model_k3 = kmeans3.fit(cluster_final_data)\n",
    "model_k2 = kmeans2.fit(cluster_final_data)\n",
    " \n",
    "wssse_k3 = model_k3.computeCost(cluster_final_data)\n",
    "wssse_k2 = model_k2.computeCost(cluster_final_data)\n",
    "\n",
    "print(\"With K=3\")\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse_k3))\n",
    "print('--'*30)\n",
    "print(\"With K=2\")\n",
    "print(\"Within Set Sum of Squared Errors = \" + str(wssse_k2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the Elbow Point (WSSSE) del 2 al 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T21:05:31.723685Z",
     "start_time": "2019-08-14T21:05:27.458607Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With K=2\n",
      "Within Set Sum of Squared Errors = 601.7707512676716\n",
      "------------------------------------------------------------\n",
      "With K=3\n",
      "Within Set Sum of Squared Errors = 434.1492898715845\n",
      "------------------------------------------------------------\n",
      "With K=4\n",
      "Within Set Sum of Squared Errors = 413.50933955528234\n",
      "------------------------------------------------------------\n",
      "With K=5\n",
      "Within Set Sum of Squared Errors = 246.44966476509273\n",
      "------------------------------------------------------------\n",
      "With K=6\n",
      "Within Set Sum of Squared Errors = 232.6312088685911\n",
      "------------------------------------------------------------\n",
      "With K=7\n",
      "Within Set Sum of Squared Errors = 225.49066138623704\n",
      "------------------------------------------------------------\n",
      "With K=8\n",
      "Within Set Sum of Squared Errors = 214.28028203478016\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for k in range(2,9):\n",
    "    kmeans = KMeans(featuresCol='scaledFeatures',k=k)\n",
    "    model = kmeans.fit(cluster_final_data)\n",
    "    wssse = model.computeCost(cluster_final_data)\n",
    "    print(\"With K={}\".format(k))\n",
    "    print(\"Within Set Sum of Squared Errors = \" + str(wssse))\n",
    "    print('--'*30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los valores de WSSSE bajan continuamente y tiene un quiebre en K = 5, dado que da un mayor salto.\n",
    "\n",
    "Cantidad de hackers involucrados en el número de hacks que se han hecho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-14T21:07:11.322189Z",
     "start_time": "2019-08-14T21:07:10.574579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|prediction|count|\n",
      "+----------+-----+\n",
      "|         1|   79|\n",
      "|         3|   63|\n",
      "|         4|   88|\n",
      "|         2|   21|\n",
      "|         0|   83|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# K = 5\n",
    "kmeans5 = KMeans(featuresCol='scaledFeatures',k=5)\n",
    "model_k5 = kmeans5.fit(cluster_final_data)\n",
    "model_k5.transform(cluster_final_data).groupBy('prediction').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
